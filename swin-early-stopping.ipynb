{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"},{"sourceId":9835760,"sourceType":"datasetVersion","datasetId":6033239},{"sourceId":9835766,"sourceType":"datasetVersion","datasetId":6033242}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7659847a89e148bf899934004d2cdb56":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_364c7727e6ee4cbebb77997c15177065"],"layout":"IPY_MODEL_a024739f2d0c4e75878cc83078ce5f61"}},"bab57d59bd2045dbbed8feaca9b21520":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0900773367eb4d648d55d087eecd6ad1","placeholder":"​","style":"IPY_MODEL_363eb791ec4542ba98dc7d25ef5a4c65","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"0b172944c8154ac58da98266193c97d8":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_8a06fbd0784b4ad69ec268a6ff8ff681","placeholder":"​","style":"IPY_MODEL_46ba24d55f2748a1b96be4f8e19058f8","value":"raazhai"}},"2a2f04f051314d85aa6d77aabbda216f":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_bd0079689a6042cd819be5e1dbbfd3ce","placeholder":"​","style":"IPY_MODEL_fe83539e32d94512a3e386dff5a8d2c2","value":""}},"a7aa03bf33ee4cd49fc7e98ad1d94dbf":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_f2b3f4cbf14940829f7ad1694bb6a953","style":"IPY_MODEL_aba9eb1f91cb4ea48e773b778b09f57b","tooltip":""}},"fbb45534835943bf86c90d578572d310":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a9b6db96b54b9dafb6de0b028fb722","placeholder":"​","style":"IPY_MODEL_b4e02d8904024110991b0c291561692c","value":"\n<b>Thank You</b></center>"}},"a024739f2d0c4e75878cc83078ce5f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"0900773367eb4d648d55d087eecd6ad1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"363eb791ec4542ba98dc7d25ef5a4c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a06fbd0784b4ad69ec268a6ff8ff681":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46ba24d55f2748a1b96be4f8e19058f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd0079689a6042cd819be5e1dbbfd3ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe83539e32d94512a3e386dff5a8d2c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2b3f4cbf14940829f7ad1694bb6a953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aba9eb1f91cb4ea48e773b778b09f57b":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"e6a9b6db96b54b9dafb6de0b028fb722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4e02d8904024110991b0c291561692c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"408d204235d047e7b8c86b8a682fa9a9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec0fa97a017f492db30338bf1ac0bbd1","placeholder":"​","style":"IPY_MODEL_bb4e77c2a7ab4ede816329aff9e97824","value":"Connecting..."}},"ec0fa97a017f492db30338bf1ac0bbd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb4e77c2a7ab4ede816329aff9e97824":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"364c7727e6ee4cbebb77997c15177065":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08463dd4030e4d14aa49d3dfe9d66bdc","placeholder":"​","style":"IPY_MODEL_70928c975c194bd09aad75b5fc0ee591","value":"Kaggle credentials successfully validated."}},"08463dd4030e4d14aa49d3dfe9d66bdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70928c975c194bd09aad75b5fc0ee591":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"006eb5eb5fa44c4da311b4d2fa5db520":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6367e352015b48569ae98c626cc489f2","IPY_MODEL_c08be083718544f8b5b9ba0ee703a329","IPY_MODEL_d99e71489a2242f8b93557b2a0870d8c"],"layout":"IPY_MODEL_f206498b0e3c40149c59fcf489ac4371"}},"6367e352015b48569ae98c626cc489f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0cc0347864b49fab4cbe411ad148d0b","placeholder":"​","style":"IPY_MODEL_22f68dff36624583aeb25e8f3636677f","value":"preprocessor_config.json: 100%"}},"c08be083718544f8b5b9ba0ee703a329":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2faee5e1cce04adea09cdf8b92310cac","max":255,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c19ed9ffc654d88a4cef6c3ec0e9bd3","value":255}},"d99e71489a2242f8b93557b2a0870d8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64d8fd3ca9ba47048d34e87c50743824","placeholder":"​","style":"IPY_MODEL_a1798576dd1640f78877fb80b296bbf0","value":" 255/255 [00:00&lt;00:00, 13.0kB/s]"}},"f206498b0e3c40149c59fcf489ac4371":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0cc0347864b49fab4cbe411ad148d0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f68dff36624583aeb25e8f3636677f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2faee5e1cce04adea09cdf8b92310cac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c19ed9ffc654d88a4cef6c3ec0e9bd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64d8fd3ca9ba47048d34e87c50743824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1798576dd1640f78877fb80b296bbf0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30b6b269c86b44dca6cd6050f6b67fd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7695804aeca04ba381fe79d760a46c40","IPY_MODEL_6c19575cba244e93976c58f3ba70b0c9","IPY_MODEL_66cf0e650042493f8a7ec9776d6ed74c"],"layout":"IPY_MODEL_9b8c1a617feb4b0d9824e6d9104a10e9"}},"7695804aeca04ba381fe79d760a46c40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85645b7fc6bc4f57ba9b583dc0e50c30","placeholder":"​","style":"IPY_MODEL_cce6d357548f4e7082bcfb7b7691e758","value":"config.json: 100%"}},"6c19575cba244e93976c58f3ba70b0c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d06b27129c846a6b0e8f825d2da9c74","max":71815,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae2bfa7e99ef4fe2ac326aa965745e6d","value":71815}},"66cf0e650042493f8a7ec9776d6ed74c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb43f433110e4d29b618b828a6fbda00","placeholder":"​","style":"IPY_MODEL_b9977e7a91a3440fbc14db806931c400","value":" 71.8k/71.8k [00:00&lt;00:00, 3.22MB/s]"}},"9b8c1a617feb4b0d9824e6d9104a10e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85645b7fc6bc4f57ba9b583dc0e50c30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cce6d357548f4e7082bcfb7b7691e758":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d06b27129c846a6b0e8f825d2da9c74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2bfa7e99ef4fe2ac326aa965745e6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb43f433110e4d29b618b828a6fbda00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9977e7a91a3440fbc14db806931c400":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"557258c2c0a94f86911d079c46a9208d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06dacf2f5b2e4782b3e8922acb34e25f","IPY_MODEL_c1bdedbd418c47edadbbad47c3140a28","IPY_MODEL_e68ec8a1830a40dca48afa02ee1746a1"],"layout":"IPY_MODEL_886500fec3d247d785e6c9b50ce865cd"}},"06dacf2f5b2e4782b3e8922acb34e25f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_076d718c810c4aa78dd32ab249b3b78f","placeholder":"​","style":"IPY_MODEL_a3b06604f2074b7e89d26c381f9b2ded","value":"model.safetensors: 100%"}},"c1bdedbd418c47edadbbad47c3140a28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ccd4a2e61a044dc9d0dcaed8235d629","max":351590690,"min":0,"orientation":"horizontal","style":"IPY_MODEL_515615e8f3de40139c9f4e0f7e096528","value":351590690}},"e68ec8a1830a40dca48afa02ee1746a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6b488a8be8344cbb60f2e1fe4d057ed","placeholder":"​","style":"IPY_MODEL_e61f242bae23487f845d94ddf965bbd4","value":" 352M/352M [00:01&lt;00:00, 237MB/s]"}},"886500fec3d247d785e6c9b50ce865cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"076d718c810c4aa78dd32ab249b3b78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3b06604f2074b7e89d26c381f9b2ded":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ccd4a2e61a044dc9d0dcaed8235d629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"515615e8f3de40139c9f4e0f7e096528":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6b488a8be8344cbb60f2e1fe4d057ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e61f242bae23487f845d94ddf965bbd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"7_kgjaeFb68C","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:40:29.326860Z","iopub.execute_input":"2024-11-13T14:40:29.327506Z","iopub.status.idle":"2024-11-13T14:40:29.334872Z","shell.execute_reply.started":"2024-11-13T14:40:29.327449Z","shell.execute_reply":"2024-11-13T14:40:29.333765Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"visual_taxonomy_path = \"/kaggle/input/visual-taxonomy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:40:29.557697Z","iopub.execute_input":"2024-11-13T14:40:29.558114Z","iopub.status.idle":"2024-11-13T14:40:29.562634Z","shell.execute_reply.started":"2024-11-13T14:40:29.558073Z","shell.execute_reply":"2024-11-13T14:40:29.561668Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df1 = pd.read_csv(visual_taxonomy_path+\"/train.csv\")\ndf2 = pd.read_csv(visual_taxonomy_path+\"/test.csv\")\nsample = pd.read_csv(visual_taxonomy_path+'/sample_submission.csv')\ndf1[\"id\"] = df1[\"id\"].astype(str).str.zfill(6)\ndf2[\"id\"] = df2[\"id\"].astype(str).str.zfill(6)","metadata":{"id":"LOUxI59vb68C","execution":{"iopub.status.busy":"2024-11-13T14:40:29.820036Z","iopub.execute_input":"2024-11-13T14:40:29.820979Z","iopub.status.idle":"2024-11-13T14:40:30.161952Z","shell.execute_reply.started":"2024-11-13T14:40:29.820926Z","shell.execute_reply":"2024-11-13T14:40:30.160911Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df1['image_path'] = df1['id'].apply(lambda x: visual_taxonomy_path+f\"/train_images/{str(x).zfill(6)}.jpg\")","metadata":{"id":"ku_ADXRUb68C","execution":{"iopub.status.busy":"2024-11-13T14:40:30.165003Z","iopub.execute_input":"2024-11-13T14:40:30.165641Z","iopub.status.idle":"2024-11-13T14:40:30.223666Z","shell.execute_reply.started":"2024-11-13T14:40:30.165596Z","shell.execute_reply":"2024-11-13T14:40:30.222617Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Calculate the number of rows to remove based on the condition\nrem_row = df1.index[df1.apply(lambda row: row.isna().sum() > 13 - row['len'], axis=1)].tolist()","metadata":{"id":"EtUDSh9mb68D","execution":{"iopub.status.busy":"2024-11-13T14:40:30.610742Z","iopub.execute_input":"2024-11-13T14:40:30.611629Z","iopub.status.idle":"2024-11-13T14:40:36.397300Z","shell.execute_reply.started":"2024-11-13T14:40:30.611588Z","shell.execute_reply":"2024-11-13T14:40:36.396499Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# rem_row = []\n# for i in range(df1.shape[0]):\n#     if df1.iloc[i,:].isna().sum() > 11 - df1.loc[i,'len']:\n#         rem_row.append(i)","metadata":{"id":"B9gd0qF_b68D","execution":{"iopub.status.busy":"2024-11-13T14:40:36.398731Z","iopub.execute_input":"2024-11-13T14:40:36.399015Z","iopub.status.idle":"2024-11-13T14:40:36.403159Z","shell.execute_reply.started":"2024-11-13T14:40:36.398985Z","shell.execute_reply":"2024-11-13T14:40:36.402216Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df1.drop(rem_row, axis = 0, inplace = True)\ndf1.reset_index(drop= True, inplace = True)","metadata":{"id":"Mm5SSw4qb68D","execution":{"iopub.status.busy":"2024-11-13T14:40:36.404240Z","iopub.execute_input":"2024-11-13T14:40:36.404510Z","iopub.status.idle":"2024-11-13T14:40:36.434172Z","shell.execute_reply.started":"2024-11-13T14:40:36.404480Z","shell.execute_reply":"2024-11-13T14:40:36.433404Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# for i in range(df1.shape[0]):\n#     if df1.iloc[i,:].isna().sum() == 10 - df1.loc[i,'len']:\n#         df1.iloc[i,:] = df1.iloc[i,:].fillna(\"extra\")","metadata":{"id":"75hBdL1nb68E","execution":{"iopub.status.busy":"2024-11-13T14:40:36.436061Z","iopub.execute_input":"2024-11-13T14:40:36.436349Z","iopub.status.idle":"2024-11-13T14:40:36.440002Z","shell.execute_reply.started":"2024-11-13T14:40:36.436318Z","shell.execute_reply":"2024-11-13T14:40:36.439147Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df1","metadata":{"id":"n_q2mn8_b68E","outputId":"367da543-86b2-4c66-df7c-4a261e700e2a","colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.status.busy":"2024-11-13T14:40:36.441038Z","iopub.execute_input":"2024-11-13T14:40:36.441311Z","iopub.status.idle":"2024-11-13T14:40:36.469548Z","shell.execute_reply.started":"2024-11-13T14:40:36.441282Z","shell.execute_reply":"2024-11-13T14:40:36.468835Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           id             Category  len      attr_1   attr_2   attr_3  \\\n0      000000          Men Tshirts    5     default    round  printed   \n1      000001          Men Tshirts    5  multicolor     polo    solid   \n2      000002          Men Tshirts    5     default     polo    solid   \n3      000003          Men Tshirts    5  multicolor     polo    solid   \n4      000004          Men Tshirts    5  multicolor     polo    solid   \n...       ...                  ...  ...         ...      ...      ...   \n53288  070373  Women Tops & Tunics   10        blue  regular  regular   \n53289  070374  Women Tops & Tunics   10  multicolor   fitted  regular   \n53290  070375  Women Tops & Tunics   10      yellow  regular     crop   \n53291  070376  Women Tops & Tunics   10      maroon   fitted     crop   \n53292  070378  Women Tops & Tunics   10        pink     boxy     crop   \n\n            attr_4         attr_5   attr_6      attr_7         attr_8  \\\n0          default  short sleeves      NaN         NaN            NaN   \n1            solid  short sleeves      NaN         NaN            NaN   \n2            solid  short sleeves      NaN         NaN            NaN   \n3            solid  short sleeves      NaN         NaN            NaN   \n4            solid  short sleeves      NaN         NaN            NaN   \n...            ...            ...      ...         ...            ...   \n53288   round neck         casual    solid       solid  short sleeves   \n53289  square neck         casual  printed     default  short sleeves   \n53290   round neck         casual  default     default  short sleeves   \n53291   round neck         casual    solid       solid  short sleeves   \n53292       v-neck         casual  printed  typography  short sleeves   \n\n                attr_9  attr_10  \\\n0                  NaN      NaN   \n1                  NaN      NaN   \n2                  NaN      NaN   \n3                  NaN      NaN   \n4                  NaN      NaN   \n...                ...      ...   \n53288          default  knitted   \n53289  regular sleeves  ruffles   \n53290  regular sleeves  knitted   \n53291  regular sleeves  knitted   \n53292  regular sleeves      NaN   \n\n                                              image_path  \n0      /kaggle/input/visual-taxonomy/train_images/000...  \n1      /kaggle/input/visual-taxonomy/train_images/000...  \n2      /kaggle/input/visual-taxonomy/train_images/000...  \n3      /kaggle/input/visual-taxonomy/train_images/000...  \n4      /kaggle/input/visual-taxonomy/train_images/000...  \n...                                                  ...  \n53288  /kaggle/input/visual-taxonomy/train_images/070...  \n53289  /kaggle/input/visual-taxonomy/train_images/070...  \n53290  /kaggle/input/visual-taxonomy/train_images/070...  \n53291  /kaggle/input/visual-taxonomy/train_images/070...  \n53292  /kaggle/input/visual-taxonomy/train_images/070...  \n\n[53293 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Category</th>\n      <th>len</th>\n      <th>attr_1</th>\n      <th>attr_2</th>\n      <th>attr_3</th>\n      <th>attr_4</th>\n      <th>attr_5</th>\n      <th>attr_6</th>\n      <th>attr_7</th>\n      <th>attr_8</th>\n      <th>attr_9</th>\n      <th>attr_10</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53288</th>\n      <td>070373</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>blue</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>default</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53289</th>\n      <td>070374</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>multicolor</td>\n      <td>fitted</td>\n      <td>regular</td>\n      <td>square neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>ruffles</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53290</th>\n      <td>070375</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>yellow</td>\n      <td>regular</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>default</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53291</th>\n      <td>070376</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>maroon</td>\n      <td>fitted</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53292</th>\n      <td>070378</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>pink</td>\n      <td>boxy</td>\n      <td>crop</td>\n      <td>v-neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>typography</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n  </tbody>\n</table>\n<p>53293 rows × 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef apply_knn_imputer_categorical(df_temp, n_neighbors=25):\n\n    max_len = df_temp['len'].max()\n    useful_columns = [f'attr_{i}' for i in range(1, max_len + 1)]\n\n    encoder = OrdinalEncoder()\n    df_encoded = df_temp.copy()\n#     print(df_temp[useful_columns])\n    df_encoded[useful_columns] = encoder.fit_transform(df_temp[useful_columns])\n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    df_imputed_array = imputer.fit_transform(df_encoded[useful_columns])\n\n    df_imputed = pd.DataFrame(df_imputed_array, columns=useful_columns, index=df_temp.index)\n#     print(df_imputed)\n\n    df_imputed[useful_columns] = df_imputed[useful_columns].round().astype(int)\n#     print(df_imputed)\n\n    df_imputed[useful_columns] = encoder.inverse_transform(df_imputed[useful_columns])\n#     print(df_imputed)\n    cols = [x for x in df_temp.columns.tolist() if x not in useful_columns]\n    df_imputed = pd.concat([df_imputed, df_temp[cols]], axis = 1)\n    df_imputed = df_imputed[df_temp.columns]\n#     print(df_imputed)\n\n    return df_imputed\n","metadata":{"id":"r8cM80rFb68F","execution":{"iopub.status.busy":"2024-11-13T14:40:36.470452Z","iopub.execute_input":"2024-11-13T14:40:36.470731Z","iopub.status.idle":"2024-11-13T14:40:37.177555Z","shell.execute_reply.started":"2024-11-13T14:40:36.470702Z","shell.execute_reply":"2024-11-13T14:40:37.176780Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create a copy of the original DataFrame to work on\ndf_imputed = df1.copy()\n\n# Get all unique categories in the dataset\ncategories = df_imputed['Category'].unique()\n\n# Loop over each category and apply the column transformer function\nfor category in categories:\n    # Step 1: Filter the DataFrame for the current category\n    df_temp = df_imputed[df_imputed['Category'] == category].copy()\n\n    # Step 2: Apply the column transformer to this category's DataFrame\n    df_temp_imputed = apply_knn_imputer_categorical(df_temp)\n\n    # Step 3: Update the original DataFrame with the imputed values\n    # Determine the useful columns for this category\n    useful_columns = [f'attr_{i}' for i in range(1, df_temp['len'].max() + 1)]\n\n    # Assign the imputed values back to the original DataFrame\n    df_imputed.loc[df_temp.index, useful_columns] = df_temp_imputed[useful_columns]\n\n# Now, df_imputed contains the imputed values for all categories","metadata":{"id":"txa61h6xb68F","execution":{"iopub.status.busy":"2024-11-13T14:40:37.178784Z","iopub.execute_input":"2024-11-13T14:40:37.179279Z","iopub.status.idle":"2024-11-13T14:41:06.205630Z","shell.execute_reply.started":"2024-11-13T14:40:37.179237Z","shell.execute_reply":"2024-11-13T14:41:06.204814Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df_imputed","metadata":{"id":"dDW8lnvOb68F","outputId":"e0822219-e5fd-4489-8c9a-200f13cce881","colab":{"base_uri":"https://localhost:8080/","height":597},"execution":{"iopub.status.busy":"2024-11-13T14:41:06.206954Z","iopub.execute_input":"2024-11-13T14:41:06.207315Z","iopub.status.idle":"2024-11-13T14:41:06.226695Z","shell.execute_reply.started":"2024-11-13T14:41:06.207275Z","shell.execute_reply":"2024-11-13T14:41:06.225868Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"           id             Category  len      attr_1   attr_2   attr_3  \\\n0      000000          Men Tshirts    5     default    round  printed   \n1      000001          Men Tshirts    5  multicolor     polo    solid   \n2      000002          Men Tshirts    5     default     polo    solid   \n3      000003          Men Tshirts    5  multicolor     polo    solid   \n4      000004          Men Tshirts    5  multicolor     polo    solid   \n...       ...                  ...  ...         ...      ...      ...   \n53288  070373  Women Tops & Tunics   10        blue  regular  regular   \n53289  070374  Women Tops & Tunics   10  multicolor   fitted  regular   \n53290  070375  Women Tops & Tunics   10      yellow  regular     crop   \n53291  070376  Women Tops & Tunics   10      maroon   fitted     crop   \n53292  070378  Women Tops & Tunics   10        pink     boxy     crop   \n\n            attr_4         attr_5   attr_6      attr_7         attr_8  \\\n0          default  short sleeves      NaN         NaN            NaN   \n1            solid  short sleeves      NaN         NaN            NaN   \n2            solid  short sleeves      NaN         NaN            NaN   \n3            solid  short sleeves      NaN         NaN            NaN   \n4            solid  short sleeves      NaN         NaN            NaN   \n...            ...            ...      ...         ...            ...   \n53288   round neck         casual    solid       solid  short sleeves   \n53289  square neck         casual  printed     default  short sleeves   \n53290   round neck         casual  default     default  short sleeves   \n53291   round neck         casual    solid       solid  short sleeves   \n53292       v-neck         casual  printed  typography  short sleeves   \n\n                attr_9  attr_10  \\\n0                  NaN      NaN   \n1                  NaN      NaN   \n2                  NaN      NaN   \n3                  NaN      NaN   \n4                  NaN      NaN   \n...                ...      ...   \n53288          default  knitted   \n53289  regular sleeves  ruffles   \n53290  regular sleeves  knitted   \n53291  regular sleeves  knitted   \n53292  regular sleeves  knitted   \n\n                                              image_path  \n0      /kaggle/input/visual-taxonomy/train_images/000...  \n1      /kaggle/input/visual-taxonomy/train_images/000...  \n2      /kaggle/input/visual-taxonomy/train_images/000...  \n3      /kaggle/input/visual-taxonomy/train_images/000...  \n4      /kaggle/input/visual-taxonomy/train_images/000...  \n...                                                  ...  \n53288  /kaggle/input/visual-taxonomy/train_images/070...  \n53289  /kaggle/input/visual-taxonomy/train_images/070...  \n53290  /kaggle/input/visual-taxonomy/train_images/070...  \n53291  /kaggle/input/visual-taxonomy/train_images/070...  \n53292  /kaggle/input/visual-taxonomy/train_images/070...  \n\n[53293 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Category</th>\n      <th>len</th>\n      <th>attr_1</th>\n      <th>attr_2</th>\n      <th>attr_3</th>\n      <th>attr_4</th>\n      <th>attr_5</th>\n      <th>attr_6</th>\n      <th>attr_7</th>\n      <th>attr_8</th>\n      <th>attr_9</th>\n      <th>attr_10</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53288</th>\n      <td>070373</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>blue</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>default</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53289</th>\n      <td>070374</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>multicolor</td>\n      <td>fitted</td>\n      <td>regular</td>\n      <td>square neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>ruffles</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53290</th>\n      <td>070375</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>yellow</td>\n      <td>regular</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>default</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53291</th>\n      <td>070376</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>maroon</td>\n      <td>fitted</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n    <tr>\n      <th>53292</th>\n      <td>070378</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>pink</td>\n      <td>boxy</td>\n      <td>crop</td>\n      <td>v-neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>typography</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>knitted</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/070...</td>\n    </tr>\n  </tbody>\n</table>\n<p>53293 rows × 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\n# Dictionary to hold encoders and data for each category\nencoders = {}\ncategory_dfs_encoded = {}\n\n# Function to one-hot encode the attributes and store the encoder\ndef one_hot_encode_category(df, category, num_attributes):\n    # Initialize OneHotEncoder\n    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n    # Select the useful columns (attributes)\n    useful_columns = [f'attr_{i}' for i in range(1, num_attributes + 1)]\n\n    # Fit and transform the attributes\n    ohe_encoded = ohe.fit_transform(df[useful_columns])\n\n    # Store the encoder for later decoding\n    encoders[category] = ohe\n\n    # Combine the encoded attributes with id, Category, and image_path\n    one_hot_df = pd.concat([df[['id', 'Category', 'image_path']].reset_index(drop=True),\n                            pd.DataFrame(ohe_encoded, columns=ohe.get_feature_names_out())], axis=1)\n\n    return one_hot_df\n\n# Split the data by category and apply one-hot encoding\ncategory_attributes = {\n    'Men Tshirts': 5,\n    'Sarees': 10,\n    'Kurtis': 9,\n    'Women Tshirts': 8,\n    'Women Tops & Tunics': 10\n}\n\nfor category, num_attributes in category_attributes.items():\n    # Filter the DataFrame by category\n    df_temp = df_imputed[df_imputed['Category'] == category].copy()\n\n    # Apply one-hot encoding\n    df_encoded = one_hot_encode_category(df_temp, category, num_attributes)\n\n    # Store the one-hot encoded DataFrame in a dictionary\n    category_dfs_encoded[category] = df_encoded\n\n# Example: Access the one-hot encoded DataFrames for each category\ndf_men_tshirts_encoded = category_dfs_encoded['Men Tshirts']\ndf_sarees_encoded = category_dfs_encoded['Sarees']\ndf_kurtis_encoded = category_dfs_encoded['Kurtis']\ndf_women_tshirts_encoded = category_dfs_encoded['Women Tshirts']\ndf_women_tunics_encoded = category_dfs_encoded['Women Tops & Tunics']\n\n# Print the shape of the one-hot encoded DataFrame for one category\nprint(f\"Shape of Men Tshirts encoded data: {df_men_tshirts_encoded.shape}\")\nprint(f\"Shape of Sarees encoded data: {df_sarees_encoded.shape}\")\nprint(f\"Shape of Kurtis encoded data: {df_kurtis_encoded.shape}\")\nprint(f\"Shape of Women Tshirts encoded data: {df_women_tshirts_encoded.shape}\")\nprint(f\"Shape of Women Tunics encoded data: {df_women_tunics_encoded.shape}\")\n\n# Display the first few rows of the encoded data for \"Men Tshirts\"\nprint(df_kurtis_encoded.head())","metadata":{"id":"wLbd8Wpjb68G","outputId":"bc7fbeb0-710e-48b8-d3b4-1e89658b71d1","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-11-13T14:41:06.229482Z","iopub.execute_input":"2024-11-13T14:41:06.229781Z","iopub.status.idle":"2024-11-13T14:41:06.493230Z","shell.execute_reply.started":"2024-11-13T14:41:06.229750Z","shell.execute_reply":"2024-11-13T14:41:06.492274Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape of Men Tshirts encoded data: (6724, 16)\nShape of Sarees encoded data: (16098, 51)\nShape of Kurtis encoded data: (4330, 33)\nShape of Women Tshirts encoded data: (15565, 32)\nShape of Women Tunics encoded data: (10576, 53)\n       id Category                                         image_path  \\\n0  025778   Kurtis  /kaggle/input/visual-taxonomy/train_images/025...   \n1  025779   Kurtis  /kaggle/input/visual-taxonomy/train_images/025...   \n2  025780   Kurtis  /kaggle/input/visual-taxonomy/train_images/025...   \n3  025781   Kurtis  /kaggle/input/visual-taxonomy/train_images/025...   \n4  025782   Kurtis  /kaggle/input/visual-taxonomy/train_images/025...   \n\n   attr_1_black  attr_1_blue  attr_1_green  attr_1_grey  attr_1_maroon  \\\n0           1.0          0.0           0.0          0.0            0.0   \n1           0.0          0.0           0.0          0.0            0.0   \n2           0.0          0.0           0.0          0.0            0.0   \n3           0.0          0.0           0.0          0.0            0.0   \n4           1.0          0.0           0.0          0.0            0.0   \n\n   attr_1_multicolor  attr_1_navy blue  ...  attr_5_net  attr_6_default  \\\n0                0.0               0.0  ...         1.0             0.0   \n1                0.0               0.0  ...         0.0             1.0   \n2                0.0               0.0  ...         0.0             1.0   \n3                0.0               1.0  ...         0.0             1.0   \n4                0.0               0.0  ...         0.0             1.0   \n\n   attr_6_solid  attr_7_default  attr_7_solid  attr_8_short sleeves  \\\n0           1.0             0.0           1.0                   0.0   \n1           0.0             1.0           0.0                   0.0   \n2           0.0             1.0           0.0                   0.0   \n3           0.0             1.0           0.0                   0.0   \n4           0.0             1.0           0.0                   0.0   \n\n   attr_8_sleeveless  attr_8_three-quarter sleeves  attr_9_regular  \\\n0                0.0                           1.0             1.0   \n1                0.0                           1.0             1.0   \n2                0.0                           1.0             1.0   \n3                0.0                           1.0             1.0   \n4                0.0                           1.0             1.0   \n\n   attr_9_sleeveless  \n0                0.0  \n1                0.0  \n2                0.0  \n3                0.0  \n4                0.0  \n\n[5 rows x 33 columns]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"category_dfs_encoded['Men Tshirts']","metadata":{"id":"xEgUuRLUb68G","outputId":"bfb10466-231a-4ffa-dc13-a63ccb3fcb81","colab":{"base_uri":"https://localhost:8080/","height":634},"execution":{"iopub.status.busy":"2024-11-13T14:41:06.494458Z","iopub.execute_input":"2024-11-13T14:41:06.494852Z","iopub.status.idle":"2024-11-13T14:41:06.526431Z","shell.execute_reply.started":"2024-11-13T14:41:06.494804Z","shell.execute_reply":"2024-11-13T14:41:06.525238Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"          id     Category                                         image_path  \\\n0     000000  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/000...   \n1     000001  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/000...   \n2     000002  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/000...   \n3     000003  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/000...   \n4     000004  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/000...   \n...      ...          ...                                                ...   \n6719  007401  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/007...   \n6720  007412  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/007...   \n6721  007417  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/007...   \n6722  007419  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/007...   \n6723  007426  Men Tshirts  /kaggle/input/visual-taxonomy/train_images/007...   \n\n      attr_1_black  attr_1_default  attr_1_multicolor  attr_1_white  \\\n0              0.0             1.0                0.0           0.0   \n1              0.0             0.0                1.0           0.0   \n2              0.0             1.0                0.0           0.0   \n3              0.0             0.0                1.0           0.0   \n4              0.0             0.0                1.0           0.0   \n...            ...             ...                ...           ...   \n6719           0.0             0.0                1.0           0.0   \n6720           0.0             0.0                1.0           0.0   \n6721           0.0             0.0                1.0           0.0   \n6722           1.0             0.0                0.0           0.0   \n6723           1.0             0.0                0.0           0.0   \n\n      attr_2_polo  attr_2_round  attr_3_printed  attr_3_solid  attr_4_default  \\\n0             0.0           1.0             1.0           0.0             1.0   \n1             1.0           0.0             0.0           1.0             0.0   \n2             1.0           0.0             0.0           1.0             0.0   \n3             1.0           0.0             0.0           1.0             0.0   \n4             1.0           0.0             0.0           1.0             0.0   \n...           ...           ...             ...           ...             ...   \n6719          0.0           1.0             1.0           0.0             1.0   \n6720          0.0           1.0             1.0           0.0             1.0   \n6721          0.0           1.0             1.0           0.0             1.0   \n6722          0.0           1.0             1.0           0.0             1.0   \n6723          0.0           1.0             1.0           0.0             1.0   \n\n      attr_4_solid  attr_4_typography  attr_5_long sleeves  \\\n0              0.0                0.0                  0.0   \n1              1.0                0.0                  0.0   \n2              1.0                0.0                  0.0   \n3              1.0                0.0                  0.0   \n4              1.0                0.0                  0.0   \n...            ...                ...                  ...   \n6719           0.0                0.0                  0.0   \n6720           0.0                0.0                  0.0   \n6721           0.0                0.0                  0.0   \n6722           0.0                0.0                  0.0   \n6723           0.0                0.0                  0.0   \n\n      attr_5_short sleeves  \n0                      1.0  \n1                      1.0  \n2                      1.0  \n3                      1.0  \n4                      1.0  \n...                    ...  \n6719                   1.0  \n6720                   1.0  \n6721                   1.0  \n6722                   1.0  \n6723                   1.0  \n\n[6724 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Category</th>\n      <th>image_path</th>\n      <th>attr_1_black</th>\n      <th>attr_1_default</th>\n      <th>attr_1_multicolor</th>\n      <th>attr_1_white</th>\n      <th>attr_2_polo</th>\n      <th>attr_2_round</th>\n      <th>attr_3_printed</th>\n      <th>attr_3_solid</th>\n      <th>attr_4_default</th>\n      <th>attr_4_solid</th>\n      <th>attr_4_typography</th>\n      <th>attr_5_long sleeves</th>\n      <th>attr_5_short sleeves</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/000...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6719</th>\n      <td>007401</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/007...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6720</th>\n      <td>007412</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/007...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6721</th>\n      <td>007417</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/007...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6722</th>\n      <td>007419</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/007...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6723</th>\n      <td>007426</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/train_images/007...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6724 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df2['image_path'] = df2['id'].apply(lambda x: visual_taxonomy_path+f\"/test_images/{str(x).zfill(6)}.jpg\")\ndf2","metadata":{"id":"3bNVVM4Ab68G","outputId":"27101764-1753-4a2f-dafc-d727f574e051","colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2024-11-13T14:41:06.527912Z","iopub.execute_input":"2024-11-13T14:41:06.528578Z","iopub.status.idle":"2024-11-13T14:41:06.562374Z","shell.execute_reply.started":"2024-11-13T14:41:06.528531Z","shell.execute_reply":"2024-11-13T14:41:06.561501Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"           id             Category  \\\n0      000000          Men Tshirts   \n1      000001          Men Tshirts   \n2      000002          Men Tshirts   \n3      000003          Men Tshirts   \n4      000004          Men Tshirts   \n...       ...                  ...   \n30200  030484  Women Tops & Tunics   \n30201  030485  Women Tops & Tunics   \n30202  030486  Women Tops & Tunics   \n30203  030487  Women Tops & Tunics   \n30204  030488  Women Tops & Tunics   \n\n                                              image_path  \n0      /kaggle/input/visual-taxonomy/test_images/0000...  \n1      /kaggle/input/visual-taxonomy/test_images/0000...  \n2      /kaggle/input/visual-taxonomy/test_images/0000...  \n3      /kaggle/input/visual-taxonomy/test_images/0000...  \n4      /kaggle/input/visual-taxonomy/test_images/0000...  \n...                                                  ...  \n30200  /kaggle/input/visual-taxonomy/test_images/0304...  \n30201  /kaggle/input/visual-taxonomy/test_images/0304...  \n30202  /kaggle/input/visual-taxonomy/test_images/0304...  \n30203  /kaggle/input/visual-taxonomy/test_images/0304...  \n30204  /kaggle/input/visual-taxonomy/test_images/0304...  \n\n[30205 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Category</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0000...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0000...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0000...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004</td>\n      <td>Men Tshirts</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0000...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30200</th>\n      <td>030484</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0304...</td>\n    </tr>\n    <tr>\n      <th>30201</th>\n      <td>030485</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0304...</td>\n    </tr>\n    <tr>\n      <th>30202</th>\n      <td>030486</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0304...</td>\n    </tr>\n    <tr>\n      <th>30203</th>\n      <td>030487</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0304...</td>\n    </tr>\n    <tr>\n      <th>30204</th>\n      <td>030488</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>/kaggle/input/visual-taxonomy/test_images/0304...</td>\n    </tr>\n  </tbody>\n</table>\n<p>30205 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"test_df = df2.copy()","metadata":{"id":"xZ3jLlVQb68G","execution":{"iopub.status.busy":"2024-11-13T14:41:06.563710Z","iopub.execute_input":"2024-11-13T14:41:06.564029Z","iopub.status.idle":"2024-11-13T14:41:06.570277Z","shell.execute_reply.started":"2024-11-13T14:41:06.563997Z","shell.execute_reply":"2024-11-13T14:41:06.569315Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom transformers import SwinForImageClassification, AutoFeatureExtractor\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch.optim as optim\nimport torch.nn as nn\n\n# Load the Swin feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/swin-base-patch4-window7-224')\n\n# Set device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"mtJ6XuGcd84w","outputId":"8fcf1fd5-03d8-4a27-9672-b3188f2ad47e","colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["006eb5eb5fa44c4da311b4d2fa5db520","6367e352015b48569ae98c626cc489f2","c08be083718544f8b5b9ba0ee703a329","d99e71489a2242f8b93557b2a0870d8c","f206498b0e3c40149c59fcf489ac4371","b0cc0347864b49fab4cbe411ad148d0b","22f68dff36624583aeb25e8f3636677f","2faee5e1cce04adea09cdf8b92310cac","2c19ed9ffc654d88a4cef6c3ec0e9bd3","64d8fd3ca9ba47048d34e87c50743824","a1798576dd1640f78877fb80b296bbf0"]},"execution":{"iopub.status.busy":"2024-11-13T14:41:06.571540Z","iopub.execute_input":"2024-11-13T14:41:06.571983Z","iopub.status.idle":"2024-11-13T14:41:24.979378Z","shell.execute_reply.started":"2024-11-13T14:41:06.571941Z","shell.execute_reply":"2024-11-13T14:41:24.978334Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec4988bfca84a53914fe0a5315478fb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class ProductDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['image_path']\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        labels = row.iloc[3:].values.astype(float)  # Starting from 4th column onwards (attributes)\n        return image, torch.tensor(labels, dtype=torch.float32)\n\n# Apply Swin feature extractor transformations to the dataset\ndef swin_transform(image):\n    return feature_extractor(images=image, return_tensors=\"pt\").pixel_values[0]\n\n# Prepare dataset (same as before)\ndef prepare_dataset(df):\n    return ProductDataset(df, transform=swin_transform)\n\n# Initialize the Swin model for multi-label classification\n\n\ndef create_swin_model(num_labels):\n    model = SwinForImageClassification.from_pretrained(\n        'microsoft/swin-base-patch4-window7-224',\n        num_labels=num_labels,\n        ignore_mismatched_sizes=True\n    )\n    # Adjust the classifier for the number of labels\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(model.config.hidden_size, num_labels)\n    )\n    return model\n\n# Training function (similar to your existing one)\ndef train_swin_model(model, dataloader, num_epochs=1):\n    model = model.to(device)\n    criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images).logits  # Forward pass\n            loss = criterion(outputs, labels)\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n            running_loss += loss.item()\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n\n    return model\n\n# Rename this function and reuse it as in your pipeline\ndef create_and_train_swin_model_for_category(df_encoded, category_name, num_epochs=3):\n    num_labels = df_encoded.shape[1] - 3  # Calculate based on columns in df_encoded\n    swin_model = create_swin_model(num_labels)\n    train_dataset = prepare_dataset(df_encoded)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    trained_model = train_swin_model(swin_model, train_loader, num_epochs)\n\n    # Save the trained model\n    model_filename = f'swin_model_{category_name}.pth'\n    torch.save(trained_model.state_dict(), model_filename)\n    print(f\"Model for {category_name} saved as {model_filename}\")\n\n    return trained_model\n","metadata":{"id":"Wu2w9es-d6wH","execution":{"iopub.status.busy":"2024-11-13T14:41:24.980810Z","iopub.execute_input":"2024-11-13T14:41:24.981421Z","iopub.status.idle":"2024-11-13T14:41:24.997298Z","shell.execute_reply.started":"2024-11-13T14:41:24.981385Z","shell.execute_reply":"2024-11-13T14:41:24.996157Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def test_swin_model(df_test, category_name, num_labels):\n    test_dataset = prepare_dataset(df_test)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    # Load the trained Swin model\n    model = create_swin_model(num_labels)\n    model.load_state_dict(torch.load(f'swin_model_{category_name}.pth'))\n    model = model.to(device)\n    model.eval()\n\n    # Make predictions\n    predictions = []\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            outputs = model(images).logits\n            preds = torch.sigmoid(outputs)\n            predictions.append(preds.cpu().numpy())\n\n    return np.concatenate(predictions, axis=0)\n","metadata":{"id":"NPmLABYseubq","execution":{"iopub.status.busy":"2024-11-13T14:41:24.998607Z","iopub.execute_input":"2024-11-13T14:41:24.999170Z","iopub.status.idle":"2024-11-13T14:41:25.034461Z","shell.execute_reply.started":"2024-11-13T14:41:24.999126Z","shell.execute_reply":"2024-11-13T14:41:25.033581Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Decode predictions using the OneHotEncoder dynamically based on the category and number of attributes\ndef decode_predictions(predictions, encoder):\n    decoded_predictions = []\n\n    for pred in predictions:\n        decoded = []\n        val = 0\n        for i in range(len(encoder.categories_)):\n            # Get the probabilities for the current attribute (i-th group of one-hot encoded columns)\n            attr_probs = pred[val : val + len(encoder.categories_[i])]\n            val = val + len(encoder.categories_[i])\n\n            # Ensure that we pick the category with the highest probability, even if all are below the threshold\n            max_index = np.argmax(attr_probs)  # Find the index of the highest probability\n\n            decoded.append(encoder.categories_[i][max_index])  # Use the encoder to map back the category\n\n        decoded_predictions.append(decoded)\n\n    return np.array(decoded_predictions)\n\ndef prepare_submission(df_test, decoded_attributes, category_name, num_attributes):\n    # Prepare the submission data\n    submission_data = {\n        'id': df_test['id'].values  # Include the 'id' column\n    }\n\n    # Get the actual number of decoded attributes from the shape of decoded_attributes\n    actual_num_attributes = decoded_attributes.shape[1]\n\n    # Dynamically add attribute columns to the dictionary based on the actual number of decoded attributes\n    for i in range(1, actual_num_attributes + 1):\n        submission_data[f'attr_{i}'] = decoded_attributes[:, i - 1]  # Adjust for zero-indexing in arrays\n\n    # Convert the dictionary to a DataFrame\n    submission_df = pd.DataFrame(submission_data)\n\n    # Save the submission DataFrame to a CSV file\n    submission_filename = f\"{category_name}_predictions.csv\"\n    submission_df.to_csv(submission_filename, index=False)\n    print(f\"Submission file saved: {submission_filename}\")\n\n\n# Generalized function to handle decoding and submission for any category\ndef decode_and_save_submission(predictions, df_test, encoder, category_name, num_attributes):\n    # Convert the predictions array\n    pred_array = predictions\n\n    # Decode the predictions using the stored OneHotEncoder\n    decoded_attributes = decode_predictions(pred_array, encoder)\n\n    # Prepare the submission DataFrame\n    submission_df = prepare_submission(df_test, decoded_attributes, num_attributes)\n\n    # Save the submission DataFrame to CSV\n    submission_file = f'{category_name}_predictions.csv'\n    submission_df.to_csv(submission_file, index=False)\n\n    print(f\"Submission file for {category_name} saved as {submission_file}\")\n\n    return submission_df","metadata":{"id":"bJ6eJjxZb68H","execution":{"iopub.status.busy":"2024-11-13T14:41:25.035708Z","iopub.execute_input":"2024-11-13T14:41:25.036052Z","iopub.status.idle":"2024-11-13T14:41:25.048161Z","shell.execute_reply.started":"2024-11-13T14:41:25.036018Z","shell.execute_reply":"2024-11-13T14:41:25.047283Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Function to save the trained model for a specific category\ndef save_model(trained_model, category_name):\n    model_filename = f'skim_model{category_name}.pth'\n    torch.save(trained_model.state_dict(), model_filename)\n    print(f\"Model for {category_name} saved as {model_filename}\")","metadata":{"id":"inz3qvoQb68H","execution":{"iopub.status.busy":"2024-11-13T14:41:25.049209Z","iopub.execute_input":"2024-11-13T14:41:25.049494Z","iopub.status.idle":"2024-11-13T14:41:25.063178Z","shell.execute_reply.started":"2024-11-13T14:41:25.049463Z","shell.execute_reply":"2024-11-13T14:41:25.062279Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']\n\ndef pipeline(categories, category_dfs_encoded, test_df, num_epoch=3):\n    for category in categories:\n        print(f\"\\nProcessing category: {category}\")\n\n        df_encoded = category_dfs_encoded[category]\n\n        # Create and train the model for this category\n        num_labels = df_encoded.shape[1] - 3\n        trained_model = create_and_train_swin_model_for_category(df_encoded, category, num_epoch)\n\n        # Save and test the model\n        save_model(trained_model, category)\n        df_test_category = test_df[test_df['Category'] == category].copy()\n\n\n        # Decode predictions and prepare submission as in your original code\n\n\n        df_test_category['image_path'] = df_test_category['id'].apply(lambda x: visual_taxonomy_path+f\"/test_images/{x}.jpg\")\n        test_dataset = prepare_dataset(df_test_category)  # Prepare the test dataset\n        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n        # 5. Test the model on the test data\n        predictions = test_swin_model(df_test_category, category, num_labels)  # Get predictions\n#         print(pd.DataFrame(predictions))\n\n        # 6. Decode the predictions using the stored OneHotEncoder\n        pred_array = np.concatenate(predictions, axis=0)  # Concatenate along rows\n        pred_array = pred_array.reshape(-1, num_labels)  # Reshape to (samples, num_labels)\n\n        decoded_predictions = decode_predictions(pred_array, encoders[category])\n#         print(pd.DataFrame(decoded_predictions))\n\n\n        # 7. Prepare and save the submission for this category\n        prepare_submission(df_test_category, decoded_predictions, category, num_labels)\n        # ...\n","metadata":{"id":"GxDHt-CKe2YX","execution":{"iopub.status.busy":"2024-11-13T14:41:25.064354Z","iopub.execute_input":"2024-11-13T14:41:25.064696Z","iopub.status.idle":"2024-11-13T14:41:25.074400Z","shell.execute_reply.started":"2024-11-13T14:41:25.064648Z","shell.execute_reply":"2024-11-13T14:41:25.073615Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# pipeline(categories, category_dfs_encoded, test_df, num_epoch = 10)","metadata":{"id":"F1NjUBbvb68I","outputId":"235f9634-06c5-4dd6-c8a6-8824ace61d9a","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["30b6b269c86b44dca6cd6050f6b67fd7","7695804aeca04ba381fe79d760a46c40","6c19575cba244e93976c58f3ba70b0c9","66cf0e650042493f8a7ec9776d6ed74c","9b8c1a617feb4b0d9824e6d9104a10e9","85645b7fc6bc4f57ba9b583dc0e50c30","cce6d357548f4e7082bcfb7b7691e758","3d06b27129c846a6b0e8f825d2da9c74","ae2bfa7e99ef4fe2ac326aa965745e6d","fb43f433110e4d29b618b828a6fbda00","b9977e7a91a3440fbc14db806931c400","557258c2c0a94f86911d079c46a9208d","06dacf2f5b2e4782b3e8922acb34e25f","c1bdedbd418c47edadbbad47c3140a28","e68ec8a1830a40dca48afa02ee1746a1","886500fec3d247d785e6c9b50ce865cd","076d718c810c4aa78dd32ab249b3b78f","a3b06604f2074b7e89d26c381f9b2ded","0ccd4a2e61a044dc9d0dcaed8235d629","515615e8f3de40139c9f4e0f7e096528","d6b488a8be8344cbb60f2e1fe4d057ed","e61f242bae23487f845d94ddf965bbd4"]},"execution":{"iopub.status.busy":"2024-11-13T14:41:25.075516Z","iopub.execute_input":"2024-11-13T14:41:25.075882Z","iopub.status.idle":"2024-11-13T14:41:25.087207Z","shell.execute_reply.started":"2024-11-13T14:41:25.075840Z","shell.execute_reply":"2024-11-13T14:41:25.086325Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"alldf = []\nfor i in categories:\n    df = pd.read_csv(f'/kaggle/working/{i}_predictions.csv')\n    alldf.append(df)","metadata":{"id":"91rXZEDib68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.103446Z","iopub.execute_input":"2024-11-11T22:29:52.104269Z","iopub.status.idle":"2024-11-11T22:29:52.163880Z","shell.execute_reply.started":"2024-11-11T22:29:52.104197Z","shell.execute_reply":"2024-11-11T22:29:52.163021Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df_final = pd.concat(alldf, axis = 0)\ndf_final = df_final.sort_values(by = 'id')\ndf_final.reset_index(drop= True, inplace = True)","metadata":{"id":"IWgqAf5Wb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.165352Z","iopub.execute_input":"2024-11-11T22:29:52.166353Z","iopub.status.idle":"2024-11-11T22:29:52.183822Z","shell.execute_reply.started":"2024-11-11T22:29:52.166301Z","shell.execute_reply":"2024-11-11T22:29:52.182954Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"df_final = pd.concat([df_final, pd.read_csv(visual_taxonomy_path+\"/test.csv\")['Category']], axis = 1)","metadata":{"id":"PkwTFCNwb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.185597Z","iopub.execute_input":"2024-11-11T22:29:52.186086Z","iopub.status.idle":"2024-11-11T22:29:52.210844Z","shell.execute_reply.started":"2024-11-11T22:29:52.186039Z","shell.execute_reply":"2024-11-11T22:29:52.210116Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"df_final['len'] = df_final['Category'].map(category_attributes)\ndf_final = df_final[sample.columns]\ndf_final.fillna('extra', inplace = True)","metadata":{"id":"kKhBKIbUb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.211900Z","iopub.execute_input":"2024-11-11T22:29:52.212194Z","iopub.status.idle":"2024-11-11T22:29:52.268030Z","shell.execute_reply.started":"2024-11-11T22:29:52.212162Z","shell.execute_reply":"2024-11-11T22:29:52.267154Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"df_final.to_csv('meesho_submission24_10ep_4na_5knn_swin.csv', index = False)","metadata":{"id":"N7kLWSfSb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.269096Z","iopub.execute_input":"2024-11-11T22:29:52.269430Z","iopub.status.idle":"2024-11-11T22:29:52.515667Z","shell.execute_reply.started":"2024-11-11T22:29:52.269395Z","shell.execute_reply":"2024-11-11T22:29:52.514728Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"df_final","metadata":{"id":"mkfpIOErb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.516919Z","iopub.execute_input":"2024-11-11T22:29:52.517264Z","iopub.status.idle":"2024-11-11T22:29:52.538204Z","shell.execute_reply.started":"2024-11-11T22:29:52.517207Z","shell.execute_reply":"2024-11-11T22:29:52.537142Z"},"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"          id             Category  len      attr_1   attr_2   attr_3  \\\n0          0          Men Tshirts    5  multicolor    round  printed   \n1          1          Men Tshirts    5     default    round  printed   \n2          2          Men Tshirts    5       white    round  printed   \n3          3          Men Tshirts    5     default     polo    solid   \n4          4          Men Tshirts    5       black    round  printed   \n...      ...                  ...  ...         ...      ...      ...   \n30200  30484  Women Tops & Tunics   10       green     boxy     crop   \n30201  30485  Women Tops & Tunics   10     default  regular  regular   \n30202  30486  Women Tops & Tunics   10     default  regular  regular   \n30203  30487  Women Tops & Tunics   10  multicolor  regular  regular   \n30204  30488  Women Tops & Tunics   10        pink   fitted     crop   \n\n            attr_4         attr_5   attr_6      attr_7         attr_8  \\\n0          default  short sleeves    extra       extra          extra   \n1          default  short sleeves    extra       extra          extra   \n2          default  short sleeves    extra       extra          extra   \n3            solid  short sleeves    extra       extra          extra   \n4            solid  short sleeves    extra       extra          extra   \n...            ...            ...      ...         ...            ...   \n30200   round neck         casual  printed  typography  short sleeves   \n30201   round neck         casual    solid       solid  short sleeves   \n30202  square neck         casual    solid       solid  short sleeves   \n30203         high         casual  printed     default   long sleeves   \n30204      default         casual    solid       solid     sleeveless   \n\n                attr_9  attr_10  \n0                extra    extra  \n1                extra    extra  \n2                extra    extra  \n3                extra    extra  \n4                extra    extra  \n...                ...      ...  \n30200  regular sleeves  tie-ups  \n30201     puff sleeves  knitted  \n30202     puff sleeves  knitted  \n30203  regular sleeves  ruffles  \n30204          default  default  \n\n[30205 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Category</th>\n      <th>len</th>\n      <th>attr_1</th>\n      <th>attr_2</th>\n      <th>attr_3</th>\n      <th>attr_4</th>\n      <th>attr_5</th>\n      <th>attr_6</th>\n      <th>attr_7</th>\n      <th>attr_8</th>\n      <th>attr_9</th>\n      <th>attr_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>multicolor</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>white</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>short sleeves</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>default</td>\n      <td>polo</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Men Tshirts</td>\n      <td>5</td>\n      <td>black</td>\n      <td>round</td>\n      <td>printed</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n      <td>extra</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30200</th>\n      <td>30484</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>green</td>\n      <td>boxy</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>typography</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>tie-ups</td>\n    </tr>\n    <tr>\n      <th>30201</th>\n      <td>30485</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>default</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>puff sleeves</td>\n      <td>knitted</td>\n    </tr>\n    <tr>\n      <th>30202</th>\n      <td>30486</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>default</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>square neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>puff sleeves</td>\n      <td>knitted</td>\n    </tr>\n    <tr>\n      <th>30203</th>\n      <td>30487</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>multicolor</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>high</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>default</td>\n      <td>long sleeves</td>\n      <td>regular sleeves</td>\n      <td>ruffles</td>\n    </tr>\n    <tr>\n      <th>30204</th>\n      <td>30488</td>\n      <td>Women Tops &amp; Tunics</td>\n      <td>10</td>\n      <td>pink</td>\n      <td>fitted</td>\n      <td>crop</td>\n      <td>default</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>sleeveless</td>\n      <td>default</td>\n      <td>default</td>\n    </tr>\n  </tbody>\n</table>\n<p>30205 rows × 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-11-11T22:29:52.539773Z","iopub.execute_input":"2024-11-11T22:29:52.540346Z","iopub.status.idle":"2024-11-11T22:29:52.548812Z","shell.execute_reply.started":"2024-11-11T22:29:52.540281Z","shell.execute_reply":"2024-11-11T22:29:52.547785Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('meesho_submission24_10ep_4na_5knn_swin.csv')","metadata":{"id":"ehg9i0AOb68I","execution":{"iopub.status.busy":"2024-11-11T22:29:52.550050Z","iopub.execute_input":"2024-11-11T22:29:52.550422Z","iopub.status.idle":"2024-11-11T22:29:52.561630Z","shell.execute_reply.started":"2024-11-11T22:29:52.550386Z","shell.execute_reply":"2024-11-11T22:29:52.560753Z"},"trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/meesho_submission24_10ep_4na_5knn_swin.csv","text/html":"<a href='meesho_submission24_10ep_4na_5knn_swin.csv' target='_blank'>meesho_submission24_10ep_4na_5knn_swin.csv</a><br>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"id":"LD8mF2y6b68I"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"eDN1xCwqb68L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"JY97DqxRb68L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"kPQL1Hs1b68L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"uA52YdaAb68L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from transformers import ViTForImageClassification, ViTFeatureExtractor\n# from torch.utils.data import DataLoader\n# from PIL import Image\n# import pandas as pd\n# import numpy as np\n# import os\n# import torch.optim as optim\n# import torch.nn as nn\n\n# # Load the feature extractor\n# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# # Set device (GPU or CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Custom dataset class for ViT\n# class ProductDataset(torch.utils.data.Dataset):\n#     def __init__(self, dataframe, transform=None):\n#         self.dataframe = dataframe\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.dataframe)\n\n#     def __getitem__(self, idx):\n#         row = self.dataframe.iloc[idx]\n#         image_path = row['image_path']\n#         image = Image.open(image_path).convert('RGB')\n#         if self.transform:\n#             image = self.transform(image)\n#         labels = row.iloc[3:].values.astype(float)  # Starting from 4th column onwards (attributes)\n#         return image, torch.tensor(labels, dtype=torch.float32)\n\n# # Apply ViT feature extractor transformations to the dataset\n# def vit_transform(image):\n#     return feature_extractor(images=image, return_tensors=\"pt\").pixel_values[0]\n\n# # Prepare dataset\n# def prepare_dataset(df):\n#     return ProductDataset(df, transform=vit_transform)\n\n# # Initialize the ViT model for multi-label classification\n# def create_vit_model(num_labels):\n#     model = ViTForImageClassification.from_pretrained(\n#         'google/vit-base-patch16-224-in21k',\n#         num_labels=num_labels\n#     )\n#     model.classifier = nn.Sequential(\n#         nn.Dropout(p=0.3),\n#         nn.Linear(model.config.hidden_size, num_labels)\n#     )\n#     return model\n\n# def train_vit_model(model, dataloader, num_epochs=1):\n#     model = model.to(device)\n#     criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification\n#     optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n#     model.train()\n#     for epoch in range(num_epochs):\n#         running_loss = 0.0\n#         for images, labels in dataloader:\n#             images, labels = images.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             outputs = model(images).logits  # Forward pass\n#             loss = criterion(outputs, labels)\n#             loss.backward()  # Backpropagation\n#             optimizer.step()  # Update weights\n#             running_loss += loss.item()\n#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n\n#     return model\n\n# # Generalized function for creating and training the ViT model for any category\n# def create_and_train_vit_model_for_category(df_encoded, category_name, num_epochs=3):\n#     # Calculate the number of labels based on the encoded dataframe\n#     num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'\n\n#     # Create the ViT model\n#     vit_model = create_vit_model(num_labels)\n\n#     # Prepare dataset and DataLoader\n#     train_dataset = prepare_dataset(df_encoded)\n#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n#     # Train the model\n#     trained_model = train_vit_model(vit_model, train_loader, num_epochs)\n\n#     # Save the trained model\n#     model_filename = f'vit_model_{category_name}.pth'\n#     torch.save(trained_model.state_dict(), model_filename)\n#     print(f\"Model for {category_name} saved as {model_filename}\")\n\n#     return trained_model\n\n# # Function to load and test the model for a given category\n# def test_vit_model(df_test, category_name, num_labels):\n#     # Prepare test dataset\n#     test_dataset = prepare_dataset(df_test)\n#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n#     # Load the trained model\n#     model = create_vit_model(num_labels)\n#     model.load_state_dict(torch.load(f'vit_model_{category_name}.pth'))\n#     model = model.to(device)\n#     model.eval()\n\n#     # Make predictions\n#     predictions = []\n#     with torch.no_grad():\n#         for images, ids in test_loader:\n#             images = images.to(device)\n#             outputs = model(images).logits\n#             preds = torch.sigmoid(outputs)\n#             predictions.append(preds.cpu().numpy())\n\n#     return np.concatenate(predictions, axis=0)\n\n# # Decode predictions using the OneHotEncoder dynamically based on the category and number of attributes\n# def decode_predictions(predictions, encoder):\n#     decoded_predictions = []\n\n#     for pred in predictions:\n#         decoded = []\n#         val = 0\n#         for i in range(len(encoder.categories_)):\n#             # Get the probabilities for the current attribute (i-th group of one-hot encoded columns)\n#             attr_probs = pred[val : val + len(encoder.categories_[i])]\n#             val = val + len(encoder.categories_[i])\n\n#             # Ensure that we pick the category with the highest probability, even if all are below the threshold\n#             max_index = np.argmax(attr_probs)  # Find the index of the highest probability\n\n#             decoded.append(encoder.categories_[i][max_index])  # Use the encoder to map back the category\n\n#         decoded_predictions.append(decoded)\n\n#     return np.array(decoded_predictions)\n\n# def prepare_submission(df_test, decoded_attributes, category_name, num_attributes):\n#     # Prepare the submission data\n#     submission_data = {\n#         'id': df_test['id'].values  # Include the 'id' column\n#     }\n\n#     # Get the actual number of decoded attributes from the shape of decoded_attributes\n#     actual_num_attributes = decoded_attributes.shape[1]\n\n#     # Dynamically add attribute columns to the dictionary based on the actual number of decoded attributes\n#     for i in range(1, actual_num_attributes + 1):\n#         submission_data[f'attr_{i}'] = decoded_attributes[:, i - 1]  # Adjust for zero-indexing in arrays\n\n#     # Convert the dictionary to a DataFrame\n#     submission_df = pd.DataFrame(submission_data)\n\n#     # Save the submission DataFrame to a CSV file\n#     submission_filename = f\"{category_name}_predictions.csv\"\n#     submission_df.to_csv(submission_filename, index=False)\n#     print(f\"Submission file saved: {submission_filename}\")\n\n\n# # Generalized function to handle decoding and submission for any category\n# def decode_and_save_submission(predictions, df_test, encoder, category_name, num_attributes):\n#     # Convert the predictions array\n#     pred_array = predictions\n\n#     # Decode the predictions using the stored OneHotEncoder\n#     decoded_attributes = decode_predictions(pred_array, encoder)\n\n#     # Prepare the submission DataFrame\n#     submission_df = prepare_submission(df_test, decoded_attributes, num_attributes)\n\n#     # Save the submission DataFrame to CSV\n#     submission_file = f'{category_name}_predictions.csv'\n#     submission_df.to_csv(submission_file, index=False)\n\n#     print(f\"Submission file for {category_name} saved as {submission_file}\")\n\n#     return submission_df\n\n# # Function to save the trained model for a specific category\n# def save_model(trained_model, category_name):\n#     model_filename = f'vit_model_{category_name}.pth'\n#     torch.save(trained_model.state_dict(), model_filename)\n#     print(f\"Model for {category_name} saved as {model_filename}\")\n\n# categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']\n\n# def pipeline(categories, category_dfs_encoded, test_df , num_epoch = 3):\n#     # Loop over each category provided in the input\n#     for category in categories:\n#         print(f\"\\nProcessing category: {category}\")\n\n#         # 1. Prepare the dataset and dataloader for the category\n#         df_encoded = category_dfs_encoded[category]  # Get the encoded dataframe for the category\n\n#         # 2. Create and train the model for this category\n#         num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'\n#         trained_model = create_and_train_vit_model_for_category(df_encoded ,category,num_epoch)\n\n#         # 3. Save the trained model\n#         save_model(trained_model, category)\n\n#         # 4. Prepare the test data for this category\n#         df_test_category = test_df[test_df['Category'] == category].copy()\n#         df_test_category['image_path'] = df_test_category['id'].apply(lambda x: f\"/kaggle/input/visual-taxonomy/test_images/{x}.jpg\")\n#         test_dataset = prepare_dataset(df_test_category)  # Prepare the test dataset\n#         test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n#         # 5. Test the model on the test data\n#         predictions = test_vit_model(df_test_category, category, num_labels)  # Get predictions\n#         print(pd.DataFrame(predictions))\n\n#         # 6. Decode the predictions using the stored OneHotEncoder\n#         pred_array = np.concatenate(predictions, axis=0)  # Concatenate along rows\n#         pred_array = pred_array.reshape(-1, num_labels)  # Reshape to (samples, num_labels)\n\n#         decoded_predictions = decode_predictions(pred_array, encoders[category])\n#         print(pd.DataFrame(decoded_predictions))\n\n\n#         # 7. Prepare and save the submission for this category\n#         prepare_submission(df_test_category, decoded_predictions, category, num_labels)","metadata":{"id":"PT6SBExhb68L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Akf-A1HUmED8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:41:25.088273Z","iopub.execute_input":"2024-11-13T14:41:25.088634Z","iopub.status.idle":"2024-11-13T14:41:25.097088Z","shell.execute_reply.started":"2024-11-13T14:41:25.088591Z","shell.execute_reply":"2024-11-13T14:41:25.096184Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom transformers import SwinForImageClassification, AutoFeatureExtractor\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\n\n# Load the Swin feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/swin-base-patch4-window7-224')\n\n# Set device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Custom dataset class for loading images and attributes\nclass ProductDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['image_path']\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        labels = row.iloc[3:].values.astype(float)\n        return image, torch.tensor(labels, dtype=torch.float32)\n\n# Transformation function using Swin feature extractor\ndef swin_transform(image):\n    return feature_extractor(images=image, return_tensors=\"pt\").pixel_values[0]\n\n# Dataset preparation function\ndef prepare_dataset(df):\n    return ProductDataset(df, transform=swin_transform)\n\n# Model creation function for Swin with a custom classifier\ndef create_swin_model(num_labels):\n    model = SwinForImageClassification.from_pretrained(\n        'microsoft/swin-base-patch4-window7-224',\n        num_labels=num_labels,\n        ignore_mismatched_sizes=True\n    )\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(model.config.hidden_size, num_labels)\n    )\n    return model\n\n# Training function with early stopping\ndef train_swin_model_with_early_stopping(model, train_loader, val_loader, num_epochs=50, patience=5, min_delta=0.001):\n    model = model.to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n    # Early stopping parameters\n    best_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images).logits\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Validation step\n        val_loss = 0.0\n        model.eval()\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images).logits\n                val_loss += criterion(outputs, labels).item()\n\n        val_loss /= len(val_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Early stopping check\n        if val_loss < best_loss - min_delta:\n            best_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # Load the best model before returning\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    return model\n\n# Testing function for making predictions on the test set\ndef test_swin_model(df_test, category_name, num_labels):\n    test_dataset = prepare_dataset(df_test)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n    model = create_swin_model(num_labels)\n    model.load_state_dict(torch.load(f'swin_model_{category_name}.pth'))\n    model = model.to(device)\n    model.eval()\n\n    predictions = []\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            outputs = model(images).logits\n            preds = torch.sigmoid(outputs)\n            predictions.append(preds.cpu().numpy())\n\n    return np.concatenate(predictions, axis=0)\n\n# Decoding function for one-hot encoded predictions\ndef decode_predictions(predictions, encoder):\n    decoded_predictions = []\n    for pred in predictions:\n        decoded = []\n        val = 0\n        for i in range(len(encoder.categories_)):\n            attr_probs = pred[val : val + len(encoder.categories_[i])]\n            val = val + len(encoder.categories_[i])\n            max_index = np.argmax(attr_probs)\n            decoded.append(encoder.categories_[i][max_index])\n        decoded_predictions.append(decoded)\n    return np.array(decoded_predictions)\n\n# Function to prepare the submission file\ndef prepare_submission(df_test, decoded_attributes, category_name, num_attributes):\n    submission_data = {'id': df_test['id'].values}\n    for i in range(1, decoded_attributes.shape[1] + 1):\n        submission_data[f'attr_{i}'] = decoded_attributes[:, i - 1]\n    submission_df = pd.DataFrame(submission_data)\n    submission_filename = f\"{category_name}_predictions.csv\"\n    submission_df.to_csv(submission_filename, index=False)\n    print(f\"Submission file saved: {submission_filename}\")\n\n# Full pipeline function with early stopping\ndef pipeline_with_early_stopping(categories, category_dfs_encoded, test_df, encoders, num_epoch=50, patience=5, min_delta=0.001):\n    for category in categories:\n        print(f\"\\nProcessing category: {category}\")\n\n        df_encoded = category_dfs_encoded[category]\n        df_train, df_val = train_test_split(df_encoded, test_size=0.2, random_state=42)\n\n        train_dataset = prepare_dataset(df_train)\n        val_dataset = prepare_dataset(df_val)\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n        num_labels = df_encoded.shape[1] - 3\n        swin_model = create_swin_model(num_labels)\n        trained_model = train_swin_model_with_early_stopping(swin_model, train_loader, val_loader, num_epoch, patience, min_delta)\n\n        torch.save(trained_model.state_dict(), f'swin_model_{category}.pth')\n        \n        df_test_category = test_df[test_df['Category'] == category].copy()\n        predictions = test_swin_model(df_test_category, category, num_labels)\n        \n        pred_array = predictions.reshape(-1, num_labels)\n        decoded_predictions = decode_predictions(pred_array, encoders[category])\n        \n        prepare_submission(df_test_category, decoded_predictions, category, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:41:25.102747Z","iopub.execute_input":"2024-11-13T14:41:25.103047Z","iopub.status.idle":"2024-11-13T14:41:25.333869Z","shell.execute_reply.started":"2024-11-13T14:41:25.103016Z","shell.execute_reply":"2024-11-13T14:41:25.332927Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"pipeline_with_early_stopping(categories, category_dfs_encoded, test_df, encoders, num_epoch=50, patience=6, min_delta=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:41:25.335225Z","iopub.execute_input":"2024-11-13T14:41:25.335599Z","iopub.status.idle":"2024-11-13T18:45:34.318336Z","shell.execute_reply.started":"2024-11-13T14:41:25.335556Z","shell.execute_reply":"2024-11-13T18:45:34.317280Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nProcessing category: Sarees\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df05a3b7d9b44854a2ff77d1610bcd1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf94cb6198e479f93ce0f72fbebf9d2"}},"metadata":{}},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([48]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([48, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.2535, Val Loss: 0.2348\nEpoch [2/50], Train Loss: 0.2315, Val Loss: 0.2302\nEpoch [3/50], Train Loss: 0.2259, Val Loss: 0.2300\nEpoch [4/50], Train Loss: 0.2221, Val Loss: 0.2275\nEpoch [5/50], Train Loss: 0.2197, Val Loss: 0.2271\nEpoch [6/50], Train Loss: 0.2179, Val Loss: 0.2283\nEpoch [7/50], Train Loss: 0.2170, Val Loss: 0.2275\nEpoch [8/50], Train Loss: 0.2149, Val Loss: 0.2290\nEpoch [9/50], Train Loss: 0.2140, Val Loss: 0.2294\nEpoch [10/50], Train Loss: 0.2129, Val Loss: 0.2291\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1828271639.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\nSome weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([48]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([48, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_30/1828271639.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'swin_model_{category_name}.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Submission file saved: Sarees_predictions.csv\n\nProcessing category: Kurtis\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([30]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([30, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.2639, Val Loss: 0.2096\nEpoch [2/50], Train Loss: 0.1732, Val Loss: 0.1753\nEpoch [3/50], Train Loss: 0.1449, Val Loss: 0.1630\nEpoch [4/50], Train Loss: 0.1272, Val Loss: 0.1617\nEpoch [5/50], Train Loss: 0.1164, Val Loss: 0.1571\nEpoch [6/50], Train Loss: 0.1039, Val Loss: 0.1616\nEpoch [7/50], Train Loss: 0.0922, Val Loss: 0.1713\nEpoch [8/50], Train Loss: 0.0848, Val Loss: 0.1864\nEpoch [9/50], Train Loss: 0.0788, Val Loss: 0.1765\nEpoch [10/50], Train Loss: 0.0701, Val Loss: 0.1895\nEpoch [11/50], Train Loss: 0.0645, Val Loss: 0.2024\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([30]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([30, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Submission file saved: Kurtis_predictions.csv\n\nProcessing category: Women Tshirts\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([29]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([29, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.1840, Val Loss: 0.1498\nEpoch [2/50], Train Loss: 0.1330, Val Loss: 0.1341\nEpoch [3/50], Train Loss: 0.1139, Val Loss: 0.1356\nEpoch [4/50], Train Loss: 0.0999, Val Loss: 0.1368\nEpoch [5/50], Train Loss: 0.0848, Val Loss: 0.1466\nEpoch [6/50], Train Loss: 0.0721, Val Loss: 0.1465\nEpoch [7/50], Train Loss: 0.0607, Val Loss: 0.1624\nEpoch [8/50], Train Loss: 0.0500, Val Loss: 0.1753\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([29]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([29, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Submission file saved: Women Tshirts_predictions.csv\n\nProcessing category: Women Tops & Tunics\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([50]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([50, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.2324, Val Loss: 0.1730\nEpoch [2/50], Train Loss: 0.1596, Val Loss: 0.1534\nEpoch [3/50], Train Loss: 0.1349, Val Loss: 0.1490\nEpoch [4/50], Train Loss: 0.1168, Val Loss: 0.1473\nEpoch [5/50], Train Loss: 0.1001, Val Loss: 0.1498\nEpoch [6/50], Train Loss: 0.0834, Val Loss: 0.1576\nEpoch [7/50], Train Loss: 0.0734, Val Loss: 0.1597\nEpoch [8/50], Train Loss: 0.0590, Val Loss: 0.1686\nEpoch [9/50], Train Loss: 0.0509, Val Loss: 0.1785\nEpoch [10/50], Train Loss: 0.0428, Val Loss: 0.1853\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([50]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([50, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Submission file saved: Women Tops & Tunics_predictions.csv\n\nProcessing category: Men Tshirts\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([13, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.2471, Val Loss: 0.2024\nEpoch [2/50], Train Loss: 0.1882, Val Loss: 0.2072\nEpoch [3/50], Train Loss: 0.1719, Val Loss: 0.1945\nEpoch [4/50], Train Loss: 0.1567, Val Loss: 0.2019\nEpoch [5/50], Train Loss: 0.1448, Val Loss: 0.2008\nEpoch [6/50], Train Loss: 0.1341, Val Loss: 0.2103\nEpoch [7/50], Train Loss: 0.1242, Val Loss: 0.2157\nEpoch [8/50], Train Loss: 0.1144, Val Loss: 0.2476\nEpoch [9/50], Train Loss: 0.1065, Val Loss: 0.2486\nEarly stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([13, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Submission file saved: Men Tshirts_predictions.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"alldf = []\nfor i in categories:\n    df = pd.read_csv(f'/kaggle/working/{i}_predictions.csv')\n    alldf.append(df)\n    \n    \ndf_final = pd.concat(alldf, axis = 0)\ndf_final = df_final.sort_values(by = 'id')\ndf_final.reset_index(drop= True, inplace = True)\n\n\ndf_final = pd.concat([df_final, pd.read_csv(visual_taxonomy_path+\"/test.csv\")['Category']], axis = 1)\n\ndf_final['len'] = df_final['Category'].map(category_attributes)\ndf_final = df_final[sample.columns]\ndf_final.fillna('extra', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:45:34.319953Z","iopub.execute_input":"2024-11-13T18:45:34.320278Z","iopub.status.idle":"2024-11-13T18:45:34.464590Z","shell.execute_reply.started":"2024-11-13T18:45:34.320245Z","shell.execute_reply":"2024-11-13T18:45:34.463816Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"df_final.to_csv('meesho_submission33_autoepoch_3na_25knn_swin.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:45:34.548731Z","iopub.execute_input":"2024-11-13T18:45:34.549032Z","iopub.status.idle":"2024-11-13T18:45:34.784644Z","shell.execute_reply.started":"2024-11-13T18:45:34.549000Z","shell.execute_reply":"2024-11-13T18:45:34.783590Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import FileLink, display, Javascript","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:03:07.006096Z","iopub.execute_input":"2024-11-13T14:03:07.006421Z","iopub.status.idle":"2024-11-13T14:03:07.010736Z","shell.execute_reply.started":"2024-11-13T14:03:07.006388Z","shell.execute_reply":"2024-11-13T14:03:07.009774Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"csv_file_path = '/kaggle/working/meesho_submission27_autoepoch_4na_17knn_swin.csv'\nfile_link = FileLink(csv_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:04:52.674644Z","iopub.execute_input":"2024-11-13T14:04:52.675284Z","iopub.status.idle":"2024-11-13T14:04:52.679602Z","shell.execute_reply.started":"2024-11-13T14:04:52.675244Z","shell.execute_reply":"2024-11-13T14:04:52.678707Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"file_link","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:04:53.214257Z","iopub.execute_input":"2024-11-13T14:04:53.214668Z","iopub.status.idle":"2024-11-13T14:04:53.220618Z","shell.execute_reply.started":"2024-11-13T14:04:53.214632Z","shell.execute_reply":"2024-11-13T14:04:53.219620Z"},"trusted":true},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/meesho_submission27_autoepoch_4na_17knn_swin.csv","text/html":"<a href='/kaggle/working/meesho_submission27_autoepoch_4na_17knn_swin.csv' target='_blank'>/kaggle/working/meesho_submission27_autoepoch_4na_17knn_swin.csv</a><br>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"def download_file():\n    display(Javascript('window.open(\"{}\");'.format(file_link)))","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:04:54.407068Z","iopub.execute_input":"2024-11-13T14:04:54.407704Z","iopub.status.idle":"2024-11-13T14:04:54.412227Z","shell.execute_reply.started":"2024-11-13T14:04:54.407665Z","shell.execute_reply":"2024-11-13T14:04:54.411299Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"download_file()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:05:12.166701Z","iopub.execute_input":"2024-11-13T14:05:12.167303Z","iopub.status.idle":"2024-11-13T14:05:12.173142Z","shell.execute_reply.started":"2024-11-13T14:05:12.167263Z","shell.execute_reply":"2024-11-13T14:05:12.172194Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"window.open(\"/kaggle/working/meesho_submission27_autoepoch_4na_17knn_swin.csv\");"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}